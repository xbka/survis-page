@article{AHMAD201935,
title = {Hand pose estimation and tracking in real and virtual interaction:A review},
journal = {Image and Vision Computing},
volume = {89},
pages = {35-49},
year = {2019},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2019.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0262885619300861},
author = {Ammar Ahmad and Cyrille Migniot and Albert Dipanda},
keywords = {Hand tracking, Real interaction, Virtual interaction, Hand-object interdependencies},
abstract = {Vision-based 3D hand tracking is a key and popular component for interaction studies in a broad range of domains such as virtual reality (VR), augmented reality (AR) and natural human-computer interaction (HCI). While this research field has been well studied in the last decades, most approaches have considered the human hand in isolation and not in action or in interaction with the surrounding environment. Even the common collaborative and strong interactions with the other hand have been ignored. However, many of today's computer applications require more and more hand-object interactions. Furthermore, employing contextual information about the object in the hand (e.g. the shape, the texture, and the pose) can remarkably constrain the tracking problem. The most studied contextual constraints involve interaction with real objects and not with virtual objects which is still a very big challenge. The goal of this survey is to develop an up-to-date taxonomy of the state-of-the-art vision-based hand pose estimation and tracking methods with a new classification scheme: hand-object interaction constraints. This taxonomy allows us to examine the strengths and weaknesses of the current state of the art and to highlight future trends in the domain.}
}

@misc{zhang2020mediapipehandsondevicerealtime,
      title={MediaPipe Hands: On-device Real-time Hand Tracking}, 
      author={Fan Zhang and Valentin Bazarevsky and Andrey Vakunov and Andrei Tkachenka and George Sung and Chuo-Ling Chang and Matthias Grundmann},
      year={2020},
      eprint={2006.10214},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2006.10214}, 
}

@inproceedings{10.1145/2702123.2702179,
author = {Sharp, Toby and Keskin, Cem and Robertson, Duncan and Taylor, Jonathan and Shotton, Jamie and Kim, David and Rhemann, Christoph and Leichter, Ido and Vinnikov, Alon and Wei, Yichen and Freedman, Daniel and Kohli, Pushmeet and Krupka, Eyal and Fitzgibbon, Andrew and Izadi, Shahram},
title = {Accurate, Robust, and Flexible Real-time Hand Tracking},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702123.2702179},
doi = {10.1145/2702123.2702179},
abstract = {We present a new real-time hand tracking system based on a single depth camera. The system can accurately reconstruct complex hand poses across a variety of subjects. It also allows for robust tracking, rapidly recovering from any temporary failures. Most uniquely, our tracker is highly flexible, dramatically improving upon previous approaches which have focused on front-facing close-range scenarios. This flexibility opens up new possibilities for human-computer interaction with examples including tracking at distances from tens of centimeters through to several meters (for controlling the TV at a distance), supporting tracking using a moving depth camera (for mobile scenarios), and arbitrary camera placements (for VR headsets). These features are achieved through a new pipeline that combines a multi-layered discriminative reinitialization strategy for per-frame pose estimation, followed by a generative model-fitting stage. We provide extensive technical details and a detailed qualitative and quantitative analysis.},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages = {3633–3642},
numpages = {10},
keywords = {hand tracking, depth camera, computer vision},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}

@INPROCEEDINGS{6909541,
  author={Qian, Chen and Sun, Xiao and Wei, Yichen and Tang, Xiaoou and Sun, Jian},
  booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Realtime and Robust Hand Tracking from Depth}, 
  year={2014},
  volume={},
  number={},
  pages={1106-1113},
  abstract={We present a realtime hand tracking system using a depth sensor. It tracks a fully articulated hand under large viewpoints in realtime (25 FPS on a desktop without using a GPU) and with high accuracy (error below 10 mm). To our knowledge, it is the first system that achieves such robustness, accuracy, and speed simultaneously, as verified on challenging real data. Our system is made of several novel techniques. We model a hand simply using a number of spheres and define a fast cost function. Those are critical for realtime performance. We propose a hybrid method that combines gradient based and stochastic optimization methods to achieve fast convergence and good accuracy. We present new finger detection and hand initialization methods that greatly enhance the robustness of tracking.},
  keywords={Three-dimensional displays;Cost function;Thumb;Accuracy;Tracking;Iterative closest point algorithm;hand tracking;ICP;PSO},
  doi={10.1109/CVPR.2014.145},
  ISSN={1063-6919},
  month={June},
}


@INPROCEEDINGS{4284820,
  author={Fang, Yikai and Wang, Kongqiao and Cheng, Jian and Lu, Hanqing},
  booktitle={2007 IEEE International Conference on Multimedia and Expo}, 
  title={A Real-Time Hand Gesture Recognition Method}, 
  year={2007},
  volume={},
  number={},
  pages={995-998},
  abstract={Compared with the traditional interaction approaches, such as keyboard, mouse, pen, etc, vision based hand interaction is more natural and efficient. In this paper, we proposed a robust real-time hand gesture recognition method. In our method, firstly, a specific gesture is required to trigger the hand detection followed by tracking; then hand is segmented using motion and color cues; finally, in order to break the limitation of aspect ratio encountered in most of learning based hand gesture methods, the scale-space feature detection is integrated into gesture recognition. Applying the proposed method to navigation of image browsing, experimental results show that our method achieves satisfactory performance.},
  keywords={Image segmentation;Keyboards;Motion detection;Computer vision;Pervasive computing;Cameras;Pattern recognition;Mice;Robustness;Tracking},
  doi={10.1109/ICME.2007.4284820},
  ISSN={1945-788X},
  month={July},
}


@INPROCEEDINGS{6519802,
  author={Zhu, Yanmin and Yang, Zhibo and Yuan, Bo},
  booktitle={2013 International Conference on Service Sciences (ICSS)}, 
  title={Vision Based Hand Gesture Recognition}, 
  year={2013},
  volume={},
  number={},
  pages={260-265},
  abstract={Hand gesture recognition has attracted much attention from academia and industry in recent years due to its apparent superiority over traditional techniques in human-computer interaction in terms of convenience. This domain has been investigated from different perspectives among which vision based approaches provide the most natural and intuitive interfaces. This paper presents a comprehensive review on vision based hand gesture recognition, with an emphasis on dynamic hand gestures. First, a brief introduction of the basic concepts and the classification of hand gesture recognition techniques are given. Then, a number of popular related technologies and interesting applications are reviewed. Finally, we give some discussion on the current challenges and open questions in this area and point out a list of possible directions for future work.},
  keywords={Gesture recognition;Hidden Markov models;Image color analysis;Skin;Solid modeling;Image segmentation;Tracking;vision based hand gesture recognition;HCI;detection and segmentation;hand tracking;classification},
  doi={10.1109/ICSS.2013.40},
  ISSN={2165-3836},
  month={April},
}


@article{Li2017,
  title = {Hand gesture recognition based on convolution neural network},
  volume = {22},
  ISSN = {1573-7543},
  url = {http://dx.doi.org/10.1007/s10586-017-1435-x},
  DOI = {10.1007/s10586-017-1435-x},
  number = {S2},
  journal = {Cluster Computing},
  publisher = {Springer Science and Business Media LLC},
  author = {Li,  Gongfa and Tang,  Heng and Sun,  Ying and Kong,  Jianyi and Jiang,  Guozhang and Jiang,  Du and Tao,  Bo and Xu,  Shuang and Liu,  Honghai},
  year = {2017},
  month = dec,
  pages = {2719–2729}
}

@ARTICLE{9352754,
  author={Fan, Hehe and Zhuo, Tao and Yu, Xin and Yang, Yi and Kankanhalli, Mohan},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Understanding Atomic Hand-Object Interaction With Human Intention}, 
  year={2022},
  volume={32},
  number={1},
  pages={275-285},
  abstract={Hand-object interaction plays a very important role when humans manipulate objects. While existing methods focus on improving hand-object recognition with fully automatic methods, human intention has been largely neglected in the recognition process, thus leading to undesirable interaction descriptions. To better interpret human-object interaction that is aligned to human intention, we argue that a reference specifying human intention should be taken into account. Thus, we propose a new approach to represent interactions while reflecting human purpose with three key factors, i.e., hand, object and reference. Specifically, we design a pattern of < hand-object, object-reference, hand, object, reference > (HOR) to recognize intention based atomic hand-object interactions. This pattern aims to model interactions with the states of hand, object, reference and their relationships. Furthermore, we design a simple yet effective Spatially Part-based (3+1)D convolutional neural network, namely SP(3+1)D, which leverages 3D and 1D convolutions to model visual dynamics and object position changes based on our HOR, respectively. With the help of our SP(3+1)D network, the recognition results are able to indicate human purposes accurately. To evaluate the proposed method, we annotate a Something-1.3k dataset, which contains 10 atomic hand-object interactions and about 130 videos for each interaction. Experimental results on Something-1.3k demonstrate the effectiveness of our SP(3+1)D network.},
  keywords={Videos;Cognition;Pattern recognition;Three-dimensional displays;Fans;Task analysis;Neural networks;Hand-object interaction reasoning;action recognition;video analysis;deep neural networks},
  doi={10.1109/TCSVT.2021.3058688},
  ISSN={1558-2205},
  month={Jan},}

@INPROCEEDINGS{8448284,
  author={Höll, Markus and Oberweger, Markus and Arth, Clemens and Lepetit, Vincent},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Efficient Physics-Based Implementation for Realistic Hand-Object Interaction in Virtual Reality}, 
  year={2018},
  volume={},
  number={},
  pages={175-182},
  keywords={Friction;Computational modeling;Grasping;Solid modeling;Three-dimensional displays;Real-time systems;I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling-Physically-based Modeling;I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction Techniques;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality;H.5.2 [Information Interfaces and Presentation]: User Interfaces-Direct Manipulation},
  doi={10.1109/VR.2018.8448284}}

@INPROCEEDINGS{5459282,
  author={Hamer, Henning and Schindler, Konrad and Koller-Meier, Esther and Van Gool, Luc},
  booktitle={2009 IEEE 12th International Conference on Computer Vision}, 
  title={Tracking a hand manipulating an object}, 
  year={2009},
  volume={},
  number={},
  pages={1475-1482},
  abstract={We present a method for tracking a hand while it is interacting with an object. This setting is arguably the one where hand-tracking has most practical relevance, but poses significant additional challenges: strong occlusions by the object as well as self-occlusions are the norm, and classical anatomical constraints need to be softened due to the external forces between hand and object. To achieve robustness to partial occlusions, we use an individual local tracker for each segment of the articulated structure. The segments are connected in a pairwise Markov random field, which enforces the anatomical hand structure through soft constraints on the joints between adjacent segments. The most likely hand configuration is found with belief propagation. Both range and color data are used as input. Experiments are presented for synthetic data with ground truth and for real data of people manipulating objects.},
  keywords={Fingers;Computer vision;Robustness;Markov random fields;Joints;Belief propagation;Application software;Laboratories;Computer science;Tracking},
  doi={10.1109/ICCV.2009.5459282},
  ISSN={2380-7504},
  month={Sep.},}

